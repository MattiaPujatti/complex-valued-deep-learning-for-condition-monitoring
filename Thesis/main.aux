\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{trabelsi2018deep}
\citation{Messerschmitt_stationary_points}
\citation{Hualiang_nonlinear}
\citation{Dramsch_seismic}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{vii}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Intro}{vii}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Previous work}{vii}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Neuronal Synchrony}{vii}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Organization}{vii}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Complex-Valued Deep Learning}{ix}{part.1}\protected@file@percent }
\citation{stein_complex_analysis}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Complex Analysis}{xi}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Complex Numbers}{xi}{section.2.1}\protected@file@percent }
\newlabel{sec:cmplx_numbers}{{2.1}{xi}{Complex Numbers}{section.2.1}{}}
\citation{Messerschmitt_stationary_points}
\citation{kreutzdelgado2009complex}
\newlabel{obs:cmplx_mult_homothety}{{2.1.1}{xii}{}{observation.2.1.1}{}}
\newlabel{def:cmplx_der}{{2.1.3}{xii}{}{definition.2.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Complex Differentiability}{xiii}{section.2.2}\protected@file@percent }
\newlabel{sec:cmplx_differentiability}{{2.2}{xiii}{Complex Differentiability}{section.2.2}{}}
\newlabel{eq:CR_derivs}{{2.2}{xiii}{Complex Differentiability}{equation.2.2.2}{}}
\newlabel{eq:CR_derivs_chain_rule}{{2.3}{xiii}{Complex Differentiability}{equation.2.2.3}{}}
\newlabel{eq:CR_calc_identities_1}{{2.4}{xiii}{Complex Differentiability}{equation.2.2.4}{}}
\newlabel{eq:CR_calc_identities_2}{{2.5}{xiii}{Complex Differentiability}{equation.2.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Other theorems}{xiii}{section.2.3}\protected@file@percent }
\citation{Nitta_complexBP}
\newlabel{th:Liouville}{{2.3.1}{xiv}{}{theorem.2.3.1}{}}
\newlabel{th:identity}{{2.3.2}{xiv}{}{theorem.2.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Circularity}{xiv}{section.2.4}\protected@file@percent }
\citation{Virtue:EECS-2019-126}
\citation{Virtue:EECS-2019-126}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Why not to prefer real-valued network with two channels?}{xv}{section.2.5}\protected@file@percent }
\newlabel{subsec:cmplx_multiplication}{{2.5}{xv}{Complex Multiplication}{section*.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visual representation of the multiplication and the corresponding implementation in a complex-valued network (bottom left). In order to replicate the same behavior on a real-valued network, the weights' components must be replicated, or shared, in the 2-channels layer (on the right). (source:\cite  {Virtue:EECS-2019-126})\relax }}{xv}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cmplx_multiplication}{{2.1}{xv}{Visual representation of the multiplication and the corresponding implementation in a complex-valued network (bottom left). In order to replicate the same behavior on a real-valued network, the weights' components must be replicated, or shared, in the 2-channels layer (on the right). (source:\cite {Virtue:EECS-2019-126})\relax }{figure.caption.6}{}}
\citation{Virtue:EECS-2019-126}
\citation{Virtue:EECS-2019-126}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visual representation of the complex multiplication (center) compared to its real-valued implementations (1-ch left, 2-ch right). (source:\cite  {Virtue:EECS-2019-126})\relax }}{xvi}{figure.caption.7}\protected@file@percent }
\newlabel{fig:mult_dof}{{2.2}{xvi}{Visual representation of the complex multiplication (center) compared to its real-valued implementations (1-ch left, 2-ch right). (source:\cite {Virtue:EECS-2019-126})\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Extent}{xvii}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problems in the extent}{xvii}{section.3.1}\protected@file@percent }
\newlabel{sec:problems_extent}{{3.1}{xvii}{Problems in the extent}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Fundamental unit (neuron) of a complex-valued neural network.\relax }}{xvii}{figure.caption.9}\protected@file@percent }
\newlabel{fig:cmplx_neuron}{{3.1}{xvii}{Fundamental unit (neuron) of a complex-valued neural network.\relax }{figure.caption.9}{}}
\citation{Nitta_complexBP}
\citation{Nitta_complexBP}
\citation{hirose_cvnn}
\citation{amin_wirtinger}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Complex Backpropagation}{xix}{section.3.2}\protected@file@percent }
\newlabel{sec:cmplx_backpropagation}{{3.2}{xix}{Complex Backpropagation}{section.3.2}{}}
\citation{amin_wirtinger,Hualiang_nonlinear}
\citation{Virtue:EECS-2019-126}
\citation{Virtue:EECS-2019-126}
\citation{Virtue:EECS-2019-126}
\citation{Virtue:EECS-2019-126}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Steepest Complex Gradient Descent}{xx}{subsection.3.2.1}\protected@file@percent }
\newlabel{sec:steepest_cmplx_gradient_descent}{{3.2.1}{xx}{Steepest Complex Gradient Descent}{subsection.3.2.1}{}}
\newlabel{eq:cmplx_gradient_descent}{{3.1}{xx}{Steepest Complex Gradient Descent}{equation.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Complex Gradient Descent.\relax }}{xx}{figure.caption.14}\protected@file@percent }
\newlabel{fig:cmplx_gradient_descent}{{3.2}{xx}{Complex Gradient Descent.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Backpropagation with a Real-valued Loss}{xx}{subsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Forward and backward pass through a complex layer. (source \cite  {Virtue:EECS-2019-126})\relax }}{xx}{figure.caption.16}\protected@file@percent }
\newlabel{fig:visual_cmplx_backpropagation}{{3.3}{xx}{Forward and backward pass through a complex layer. (source \cite {Virtue:EECS-2019-126})\relax }{figure.caption.16}{}}
\citation{xavier_init}
\citation{he2015delving}
\citation{trabelsi2018deep}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Comparison of backpropagation calculus. (source: \cite  {Virtue:EECS-2019-126})\relax }}{xxi}{table.caption.15}\protected@file@percent }
\newlabel{tab:comparison_backpropagation}{{3.1}{xxi}{Comparison of backpropagation calculus. (source: \cite {Virtue:EECS-2019-126})\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Re-definition of the main neural network layers}{xxi}{section.3.3}\protected@file@percent }
\citation{trabelsi2018deep}
\citation{trabelsi2018deep}
\citation{trabelsi2018deep}
\citation{trabelsi2018deep}
\newlabel{eq:cmplx_convolution}{{3.2}{xxii}{Convolutional Layers}{equation.3.3.2}{}}
\citation{cogswell2016reducing}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Implementation details of the Complex Convolution (by \cite  {trabelsi2018deep}).\relax }}{xxiii}{figure.caption.19}\protected@file@percent }
\newlabel{fig:cmplx_convolution}{{3.4}{xxiii}{Implementation details of the Complex Convolution (by \cite {trabelsi2018deep}).\relax }{figure.caption.19}{}}
\newlabel{eq:cmplx_batchnorm}{{3.3}{xxiii}{Normalization Layers}{equation.3.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Complex-Valued Activation Functions}{xxiv}{section.3.4}\protected@file@percent }
\citation{Virtue:EECS-2019-126}
\citation{Nitta_complexBP}
\citation{Nitta_complexBP}
\citation{trabelsi2018deep}
\citation{guberman2016complex}
\citation{Koutsougeras_siglog}
\citation{Virtue:EECS-2019-126}
\citation{DBLP:journals/corr/ArjovskySB15}
\citation{Virtue:EECS-2019-126}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Recal of the most popular complex-valued activation functions.\relax }}{xxvi}{table.caption.28}\protected@file@percent }
\newlabel{tab:cmplx_activations}{{3.2}{xxvi}{Recal of the most popular complex-valued activation functions.\relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}JAX Implementation}{xxvi}{section.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces JAX logo.\relax }}{xxvii}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Complex-Valued vs Real Neural Networks}{xxix}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Impact of the Circularity}{xxix}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Complex vs Real Models}{xxix}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Condition Monitoring}{xxxi}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Healty-Faulty Signal Classification}{xxxiii}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}}{xxxiii}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Datasets}{xxxiii}{section.5.2}\protected@file@percent }
\citation{*}
\bibdata{bibliography}
\bibcite{trabelsi2018deep}{1}
\bibcite{Messerschmitt_stationary_points}{2}
\bibcite{Hualiang_nonlinear}{3}
\bibcite{Dramsch_seismic}{4}
\bibcite{stein_complex_analysis}{5}
\bibcite{kreutzdelgado2009complex}{6}
\bibcite{Nitta_complexBP}{7}
\bibcite{Virtue:EECS-2019-126}{8}
\bibcite{hirose_cvnn}{9}
\bibcite{amin_wirtinger}{10}
\bibcite{xavier_init}{11}
\bibcite{he2015delving}{12}
\bibcite{cogswell2016reducing}{13}
\bibcite{guberman2016complex}{14}
\bibcite{Koutsougeras_siglog}{15}
\bibcite{DBLP:journals/corr/ArjovskySB15}{16}
\bibcite{ziller2021complexvalued}{17}
\bibcite{article}{18}
\bibcite{4682548}{19}
\bibcite{virtue2017better}{20}
\bibcite{GARDNER2021116245}{21}
\bibcite{ajakan2015domainadversarial}{22}
\bibcite{ganin2016domainadversarial}{23}
\bibcite{reichert2014neuronal}{24}
\bibcite{scardapane2018complexvalued}{25}
\bibcite{barrachina2021complexvalued}{26}
\bibcite{shen2018wasserstein}{27}
\bibcite{schlomer_nico_2021_5636188}{28}
\bibcite{Farris_visual_complex_analysis}{29}
\bibstyle{ieeetr}
\citation{trabelsi2018deep}
\citation{xavier_init}
\citation{he2015delving}
\citation{xavier_init}
\citation{he2015delving}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Mathematical Proofs}{xxxvii}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:cmplx_optim}{{A}{xxxvii}{Mathematical Proofs}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Complex Weights Initialization \cite  {trabelsi2018deep}}{xxxvii}{section.A.1}\protected@file@percent }
\newlabel{app:weight_init}{{A.1}{xxxvii}{Complex Weights Initialization \cite {trabelsi2018deep}}{section.A.1}{}}
\citation{Messerschmitt_stationary_points}
\citation{MESSERSCHMITT_STATIONARY_POINTS}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Stationary points of a real-valued function of a complex variable \cite  {Messerschmitt_stationary_points}}{xxxviii}{section.A.2}\protected@file@percent }
\citation{Hualiang_nonlinear}
\citation{HUALIANG_NONLINEAR}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Steepest complex gradient descent \cite  {Hualiang_nonlinear}}{xxxix}{section.A.3}\protected@file@percent }
\gdef \@abspage@last{39}
