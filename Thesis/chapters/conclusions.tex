\documentclass[../main.tex]{subfiles}	

\begin{document}
	

\chapter*{Conclusions and Future Works}

During this thesis work, we collected the main instruments, extents and algorithms proposed during the years, and merged them in a unique and working complex-valued deep learning framework. We tried to be as coherent as possible, remaining, at the same time, rigorous from a mathematical perspective, discussing also the main obstacles that researchers encountered during their studies. The high number of tests realized, that confirmed the quality of our work, was possible thanks to the Python library we have written, that is characterized by an efficiency that the most common machine learning algorithms actually do not support for complex-valued data types.\\
Even if we didn't managed to get any definitive proof that complex models works better that their real-valued two-channels counterparts, as the theory seems to suggest, we had some clues about their higher robustness to overfitting and their increased generalization capabilities, that are the same impressions that similar works had during the years. Furthermore, we felt that the advantages brought by complex models were strongly dependent on properties of the datasets used: unluckily, all the experimental input data used during this thesis work have shown very poor correlations among real and imaginary parts. We believe the true potential of such networks, at least with respect to the two-channels approach, can arise only when there are effectively inherent correlations to learn inside data. \\
We want to insist also on the computational results achieved in the last chapter of part \ref{part:cmplx_deep_learning}, that is probably the most interesting find during this work, at least from a theoretical point of view: we found that complex and real-valued models behave differently in case of data distribution with a strong component of non-circularity. We believe that this aspect deserves further studies.\\
In the end, we have also demonstrated that is effectively possible to extend existing transfer learning algorithms, like the one based on the Wasserstein distance, to work with complex-valued data, and that researchers should focus more further in that direction, almost completely unexplored in recent years.
	
	
\end{document}